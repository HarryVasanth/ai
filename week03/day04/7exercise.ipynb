{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "# Exercise 7: Bag of Word representation\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "The goal of this exercise is to understand how to create a Bag of Word (BoW) model on a corpus of texts. More precisely we will create a labeled data set from textual data using a word count matrix.\n",
    "\n",
    "As explained in the resource, the Bag of word representation makes the assumption that the order in which the words appear in a text doesn't matter. There are different types of Bag of words representation:\n",
    "\n",
    "- Boolean: Each document is a boolean vector\n",
    "- Wordcount: Each document is a word count vector\n",
    "- TFIDF: Each document is a score vector. The score is detailed in the next exercise.\n",
    "\n",
    "The data `tweets_train.txt` contains tweets labeled with a sentiment. It gives the positivity of a tweet.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Preprocess the data using the function implemented in the previous exercise. And, using from `CountVectorizer` of scikitlearn with `max_features=500` compute the wordcount of the tweets. The output is a sparse matrix.\n",
    "\n",
    "- Check the shape of the word count matrix\n",
    "- Set **max_features** to 500 of the initial size of the dictionary.\n",
    "\n",
    "**Reminder**: Given that a data set is often described as an m x n matrix in which m is the number of rows and n is the number of columns: features. It is strongly recommended to work with m >> n. The value of the ratio depends on the signal existing in the data set and on the model complexity.\n",
    "\n",
    "2. Using from_spmatrix from scikitlearn create a DataFrame with documents in rows and dictionary in columns.\n",
    "\n",
    "|     | and | boat | compute |\n",
    "| --: | --: | ---: | ------: |\n",
    "|   0 |   0 |    2 |       0 |\n",
    "|   1 |   0 |    0 |       1 |\n",
    "|   2 |   1 |    0 |       0 |\n",
    "\n",
    "3. Create a dataframe with the labels\n",
    "\n",
    "- 1: positive\n",
    "- 0: neutral\n",
    "- -1: negative\n",
    "\n",
    "|     | target |\n",
    "| --: | -----: |\n",
    "|   0 |     -1 |\n",
    "|   1 |      0 |\n",
    "|   2 |      1 |\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-353bfe0741b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def prepare(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in words if not w in stop_words]\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for w in filtered_sentence:\n",
    "        result.append(ps.stem(w))\n",
    "    return result\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=500)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X)"
   ]
  }
 ]
}